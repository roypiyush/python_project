{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c88a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "769da96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_text = None\n",
    "with open('input.txt') as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c968ddff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique characters in the corpus is 65\n",
      "A slice of the unique characters set:\n",
      " ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(shakespeare_text))\n",
    "print ('The number of unique characters in the corpus is', len(vocab))\n",
    "print('A slice of the unique characters set:\\n', vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b805cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "# Make a copy of the unique set elements in NumPy array format for later use in the decoding the predictions\n",
    "idx2char = np.array(vocab)\n",
    "# Vectorize the text with a for loop\n",
    "text_as_int = np.array([char2idx[c] for c in shakespeare_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63d9953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) \n",
    "# for i in char_dataset.take(5): \n",
    "#   print(i.numpy())\n",
    "seq_length = 100 # The max. length for single input\n",
    "# examples_per_epoch = len(text)//(seq_length+1) # double-slash for “floor” division\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True) \n",
    "# for item in sequences.take(5): \n",
    "#   print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1aa43784",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_input_target(chunk):\n",
    "  input_text = chunk[:-1]\n",
    "  target_text = chunk[1:]\n",
    "  return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43598ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 10000 # TF shuffles the data only within buffers\n",
    "\n",
    "BATCH_SIZE = 64 # Batch size\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b20311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2c400c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbb6246f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (64, None, 256)           16640     \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (64, None, 1024)          3938304   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (64, None, 65)            66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    vocab_size = len(vocab), # no. of unique characters\n",
    "    embedding_dim=embedding_dim, # 256\n",
    "    rnn_units=rnn_units, # 1024\n",
    "    batch_size=BATCH_SIZE)  # 64 for the traning\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81e39788",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "# example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "# print(\"Prediction shape: \", example_batch_predictions.shape, \" (batch_size, sequence_length, vocab_size)\")\n",
    "# print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "956a94b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b936958f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39mas_numpy_iterator:\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(a)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not iterable"
     ]
    }
   ],
   "source": [
    "for a in dataset.as_numpy_iterator:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8006c240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "172/172 [==============================] - 267s 2s/step - loss: 2.6478\n",
      "Epoch 2/30\n",
      "172/172 [==============================] - 267s 2s/step - loss: 1.9499\n",
      "Epoch 3/30\n",
      "172/172 [==============================] - 271s 2s/step - loss: 1.6831\n",
      "Epoch 4/30\n",
      "172/172 [==============================] - 272s 2s/step - loss: 1.5377\n",
      "Epoch 5/30\n",
      "172/172 [==============================] - 276s 2s/step - loss: 1.4512\n",
      "Epoch 6/30\n",
      "172/172 [==============================] - 275s 2s/step - loss: 1.3928\n",
      "Epoch 7/30\n",
      "172/172 [==============================] - 267s 2s/step - loss: 1.3475\n",
      "Epoch 8/30\n",
      "172/172 [==============================] - 269s 2s/step - loss: 1.3082\n",
      "Epoch 9/30\n",
      "172/172 [==============================] - 272s 2s/step - loss: 1.2738\n",
      "Epoch 10/30\n",
      "172/172 [==============================] - 268s 2s/step - loss: 1.2417\n",
      "Epoch 11/30\n",
      "172/172 [==============================] - 262s 2s/step - loss: 1.2096\n",
      "Epoch 12/30\n",
      "172/172 [==============================] - 270s 2s/step - loss: 1.1772\n",
      "Epoch 13/30\n",
      "172/172 [==============================] - 266s 2s/step - loss: 1.1433\n",
      "Epoch 14/30\n",
      "172/172 [==============================] - 267s 2s/step - loss: 1.1113\n",
      "Epoch 15/30\n",
      "172/172 [==============================] - 261s 2s/step - loss: 1.0756\n",
      "Epoch 16/30\n",
      "172/172 [==============================] - 265s 2s/step - loss: 1.0400\n",
      "Epoch 17/30\n",
      "172/172 [==============================] - 271s 2s/step - loss: 1.0037\n",
      "Epoch 18/30\n",
      "172/172 [==============================] - 267s 2s/step - loss: 0.9687\n",
      "Epoch 19/30\n",
      "172/172 [==============================] - 264s 2s/step - loss: 0.9343\n",
      "Epoch 20/30\n",
      "172/172 [==============================] - 260s 2s/step - loss: 0.8998\n",
      "Epoch 21/30\n",
      "172/172 [==============================] - 274s 2s/step - loss: 0.8685\n",
      "Epoch 22/30\n",
      "172/172 [==============================] - 272s 2s/step - loss: 0.8386\n",
      "Epoch 23/30\n",
      "172/172 [==============================] - 269s 2s/step - loss: 0.8119\n",
      "Epoch 24/30\n",
      "172/172 [==============================] - 273s 2s/step - loss: 0.7884\n",
      "Epoch 25/30\n",
      "172/172 [==============================] - 280s 2s/step - loss: 0.7669\n",
      "Epoch 26/30\n",
      "172/172 [==============================] - 270s 2s/step - loss: 0.7477\n",
      "Epoch 27/30\n",
      "172/172 [==============================] - 269s 2s/step - loss: 0.7313\n",
      "Epoch 28/30\n",
      "172/172 [==============================] - 270s 2s/step - loss: 0.7169\n",
      "Epoch 29/30\n",
      "172/172 [==============================] - 275s 2s/step - loss: 0.7052\n",
      "Epoch 30/30\n",
      "172/172 [==============================] - 300s 2s/step - loss: 0.6934\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCHS = 30\n",
    "history = model.fit(dataset, \n",
    "                    epochs=EPOCHS, \n",
    "                    callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cf16eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, num_generate, temperature, start_string):\n",
    "  input_eval = [char2idx[s] for s in start_string] # string to numbers (vectorizing)\n",
    "  input_eval = tf.expand_dims(input_eval, 0) # dimension expansion\n",
    "  text_generated = [] # Empty string to store our results\n",
    "  model.reset_states() # Clears the hidden states in the RNN\n",
    "\n",
    "  for i in range(num_generate): #Run a loop for number of characters to generate\n",
    "    predictions = model(input_eval) # prediction for single character\n",
    "    predictions = tf.squeeze(predictions, 0) # remove the batch dimension\n",
    "\n",
    "    # using a categorical distribution to predict the character returned by the model\n",
    "    # higher temperature increases the probability of selecting a less likely character\n",
    "    # lower --> more predictable\n",
    "    predictions = predictions / temperature\n",
    "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "    # The predicted character as the next input to the model\n",
    "    # along with the previous hidden state\n",
    "    # So the model makes the next prediction based on the previous character\n",
    "    input_eval = tf.expand_dims([predicted_id], 0) \n",
    "    # Also devectorize the number and add to the generated text\n",
    "    text_generated.append(idx2char[predicted_id]) \n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f1f78c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (1, None, 256)            16640     \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (1, None, 1024)           3938304   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (1, None, 65)             66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3722591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Come, sir, and I dare not say he: 'fore mestrum lies you into my house;\n",
      "For both offen, ere they give;\n",
      "And, for I am a manner of mercy. And his wonder;\n",
      "Nor what am too much first and what time had made\n",
      "That thou art.\n",
      "\n",
      "AUFIDIUS:\n",
      "You have done enough, nor worse: is't my speech and\n",
      "Willought to my foe till thou do: abother,\n",
      "If he hath lost advanced. O, whilst I dur\n",
      "boy, thou art a tall fellow, and his\n",
      "school'd my wife!\n",
      "\n",
      "ANTONIO:\n",
      "His noble country's reasons of the worst\n",
      "Of Sochaid to be unclar, ev\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(\n",
    "                    model, \n",
    "                    num_generate=500, \n",
    "                    temperature=1, \n",
    "                    start_string=u\"ROMEO\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a96335fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([shakespeare_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f2ee733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[20, 6, 9, 8, 3]], ['f i r s t'], [[1]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"First\"]), tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]]), tokenizer.texts_to_sequences([\" \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06a926ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_id = len(tokenizer.word_index) # number of distinct characters\n",
    "dataset_size = tokenizer.document_count # total number of characters\n",
    "\n",
    "max_id, dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "684aee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the full text so each character is represented by its ID \n",
    "# (we subtract 1 to get IDs from 0 to 38, rather than from 1 to 39)\n",
    "\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ff78663",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a08c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1 # target = input shifted 1 character ahead \n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f74bfa15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FlatMapDataset element_spec=TensorSpec(shape=(None,), dtype=tf.int64, name=None)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aa1f47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9c9db65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.data.ops.dataset_ops.DatasetV2.counter(start=0, step=1, dtype=tf.int64, name=None)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69f632d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id], dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))])\n",
    "    \n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "013c6b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 39) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 39), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, None).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/piyushr/workspace/personal/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/piyushr/workspace/personal/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/piyushr/workspace/personal/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/piyushr/workspace/personal/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/piyushr/workspace/personal/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/piyushr/workspace/personal/venv/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 232, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential' (type Sequential).\n    \n    Input 0 of layer \"gru\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, None)\n    \n    Call arguments received by layer 'sequential' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, None), dtype=int64)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(dataset, epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n",
      "File \u001b[0;32m~/workspace/personal/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/j6/vt_gmt8x1n74q_9nccxjmp5w0000gp/T/__autograph_generated_file5d0yv10s.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/piyushr/workspace/personal/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/piyushr/workspace/personal/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/piyushr/workspace/personal/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/piyushr/workspace/personal/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/piyushr/workspace/personal/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/piyushr/workspace/personal/venv/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 232, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential' (type Sequential).\n    \n    Input 0 of layer \"gru\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, None)\n    \n    Call arguments received by layer 'sequential' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, None), dtype=int64)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ccfa0cc7987480c27b9ca0f59c3a2d59999bd31ea927a6b1a2cfba7d1e11b95"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
